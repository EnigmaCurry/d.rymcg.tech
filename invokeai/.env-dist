# The docker image to use (https://github.com/invoke-ai/InvokeAI/tags):
INVOKEAI_IMAGE=ghcr.io/invoke-ai/invokeai:v6.4.0

# The domain name for the invokeai service:
INVOKEAI_TRAEFIK_HOST=invokeai.example.com

# The name of this instance. If there is only one instance, use 'default'.
INVOKEAI_INSTANCE=

# Valid values are "cuda" (for Nvidia GPUs), "rocm" (for AMD GPUs),
# or "cpu" (for CPU only). You should only have one value here, not
# multiple values.
DOCKER_COMPOSE_PROFILES=cuda

# Filter access by IP address source range (CIDR):
##Disallow all access: 0.0.0.0/32
##Allow all access: 0.0.0.0/0
INVOKEAI_IP_SOURCERANGE=0.0.0.0/0

# HTTP Basic Authentication:
# Use `make config` to fill this in properly, or set this to blank to disable.
INVOKEAI_HTTP_AUTH=

# OAUTH2
# Set to `true` to use OpenID/OAuth2 authentication via the
# traefik-forward-auth service in d.rymcg.tech.
# Using OpenID/OAuth2 will require login to access your app,
# but it will not affect what a successfully logged-in person can do in your
# app. If your app has built-in authentication and can check the user
# header that traefik-forward-auth sends, then your app can limit what the
# logged-in person can do in the app. But if your app can't check the user
# header, or if your app doesn't have built-in authentication at all, then
# any person with an account on your Forgejo server can log into your app and
# have full access.
INVOKEAI_OAUTH2=false
# In addition to Oauth2 authentication, you can configure basic authorization
# by entering which authorization group can log into your app. You create
# groups of email addresses in the `traefik` folder by running `make groups`.
INVOKEAI_OAUTH2_AUTHORIZED_GROUP=

# Mutual TLS (mTLS):
# Set true or false. If true, all clients must present a certificate signed by Step-CA:
INVOKEAI_MTLS_AUTH=false
# Enter a comma separated list of client domains allowed to connect via mTLS.
# Wildcards are allowed and encouraged on a per-app basis:
INVOKEAI_MTLS_AUTHORIZED_CERTS=*.clients.invokeai.example.com

## GPU_DRIVER can be set to either `cuda` or `rocm` to enable GPU
## support in the container accordingly.
INVOKEAI_GPU_DRIVER=cuda

## If you are using ROCm for an AMD GPU, you will need to ensure that
## the "render" group within the container and the host system use the
## same group ID. Running `make config` will automatically `ssh` into
## the current Docker context (i.e., the host), determine the GID of
## the "render" group, and add it to your configuration. If you want
## to determine it manually, you can run `make get-render-gid` from
## this directory, or you can manually run `getent group render` on
## the host and note the GID number.
INVOKEAI_RENDER_GROUP_ID=

## Set INVOKEAI_STORE_MODELS_SEPARATELY to "false" for InvokeAI to
## store its models in the same Docker volume as the rest of its data,
## or to "true" for InvokeAI to store its models in a path on the host
## (INVOKEAI_SEPARATE_MODELS_HOST_PATH) that's bind-mounted into the
## container
INVOKEAI_STORE_MODELS_SEPARATELY=false
INVOKEAI_SEPARATE_MODELS_HOST_PATH=

INVOKEAI_CONTAINER_UID=1000
INVOKEAI_CONTAINER_GID=1000

# META:
# PREFIX=INVOKEAI
